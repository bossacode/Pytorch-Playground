{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.nn\n",
    "\n",
    "https://pytorch.org/docs/stable/nn.html\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/nn_tutorial.html\n",
    "\n",
    "https://blog.paperspace.com/pytorch-101-advanced/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Low-level vs. High-level Linear Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([48, 3])"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor(np.array([np.arange(1, 10.6, 0.2),\n",
    "                           np.arange(1, 10.6, 0.2),\n",
    "                           np.arange(1, 10.6, 0.2)]), dtype=torch.float32).t()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([48])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "y = X.matmul(torch.tensor([30.7, -17.2, 23.3])) + 19.2 + 0.5*torch.randn(X.shape[0])\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.0000, 1.0000, 1.0000],\n",
       "         [1.2000, 1.2000, 1.2000],\n",
       "         [1.4000, 1.4000, 1.4000],\n",
       "         [1.6000, 1.6000, 1.6000],\n",
       "         [1.8000, 1.8000, 1.8000],\n",
       "         [2.0000, 2.0000, 2.0000],\n",
       "         [2.2000, 2.2000, 2.2000],\n",
       "         [2.4000, 2.4000, 2.4000],\n",
       "         [2.6000, 2.6000, 2.6000],\n",
       "         [2.8000, 2.8000, 2.8000]]),\n",
       " tensor([ 55.2372,  62.9849,  70.3930,  77.2753,  85.3899,  92.4954,  99.6701,\n",
       "         106.7155, 114.5239, 122.3919]))"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:10, :], y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Low-Level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights and Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0955, 0.0385, 0.0089], requires_grad=True),\n",
       " tensor([0.6213], requires_grad=True))"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# low-level\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "weights = torch.randn(3) / math.sqrt(len(X))  # xavier initialization\n",
    "weights.requires_grad_(True)    # set require_grad after initialization bc. we don't want this included in gradient\n",
    "\n",
    "bias = torch.randn(1, requires_grad=True)\n",
    "\n",
    "weights, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_reg(X, weights=weights, bias=bias):\n",
    "    pred = X.matmul(weights) + bias\n",
    "    return pred\n",
    "\n",
    "def mse_loss(pred, y):\n",
    "    loss = torch.sum((pred - y)**2) / len(y)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 6)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 8\n",
    "\n",
    "X_batches = torch.split(X, batch_size)\n",
    "y_batches = torch.split(y, batch_size)\n",
    "\n",
    "len(X_batches), len(y_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "epoch 1 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 18928.08984375\n",
      "batch [4 / 6] loss: 39545.0234375\n",
      "batch [6 / 6] loss: 17308.240234375\n",
      "--------------------------------------------------\n",
      "epoch 2 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 953.0055541992188\n",
      "batch [4 / 6] loss: 1121.216064453125\n",
      "batch [6 / 6] loss: 300.2220458984375\n",
      "--------------------------------------------------\n",
      "epoch 3 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 171.156494140625\n",
      "batch [4 / 6] loss: 35.11738586425781\n",
      "batch [6 / 6] loss: 4.091669082641602\n",
      "--------------------------------------------------\n",
      "epoch 4 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 102.42078399658203\n",
      "batch [4 / 6] loss: 2.9902000427246094\n",
      "batch [6 / 6] loss: 25.7353572845459\n",
      "--------------------------------------------------\n",
      "epoch 5 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 92.339111328125\n",
      "batch [4 / 6] loss: 1.7991183996200562\n",
      "batch [6 / 6] loss: 31.214757919311523\n",
      "--------------------------------------------------\n",
      "epoch 6 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 90.35986328125\n",
      "batch [4 / 6] loss: 1.7117235660552979\n",
      "batch [6 / 6] loss: 32.05941390991211\n",
      "--------------------------------------------------\n",
      "epoch 7 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 89.68729400634766\n",
      "batch [4 / 6] loss: 1.6947760581970215\n",
      "batch [6 / 6] loss: 32.07575607299805\n",
      "--------------------------------------------------\n",
      "epoch 8 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 89.2336196899414\n",
      "batch [4 / 6] loss: 1.6863932609558105\n",
      "batch [6 / 6] loss: 31.953433990478516\n",
      "--------------------------------------------------\n",
      "epoch 9 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 88.8177261352539\n",
      "batch [4 / 6] loss: 1.6793941259384155\n",
      "batch [6 / 6] loss: 31.808271408081055\n",
      "--------------------------------------------------\n",
      "epoch 10 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 88.40984344482422\n",
      "batch [4 / 6] loss: 1.6726408004760742\n",
      "batch [6 / 6] loss: 31.65999984741211\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "\n",
    "def train(X_batches, y_batches, model, loss_fn, weights, bias, lr = 0.001, lr_decay = 0.99):\n",
    "    for batch_num, (X, y) in enumerate(zip(X_batches, y_batches)):\n",
    "        pred = model(X, weights, bias)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "        with torch.no_grad():   # we don't want these actions to be recorded for next calculation of the gradient\n",
    "            # SGD\n",
    "            weights -= lr * weights.grad\n",
    "            bias -= lr * bias.grad\n",
    "\n",
    "            # reset gradient\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()\n",
    "\n",
    "        lr *= lr_decay\n",
    "\n",
    "        if (batch_num + 1) % 2 == 0:\n",
    "                print(f'batch [{batch_num + 1} / {len(X_batches)}] loss: {loss.item()}')\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('-' * 50)\n",
    "    print(f'epoch {epoch + 1} / {epochs}')\n",
    "    print('-' * 50)\n",
    "\n",
    "    train(X_batches, y_batches, linear_reg, mse_loss, weights, bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. High-level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using torch.nn.functional\n",
    "\n",
    "https://pytorch.org/docs/stable/nn.functional.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(52.5023, grad_fn=<MseLossBackward0>),\n",
       " tensor(52.5023, grad_fn=<DivBackward0>))"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "loss_fn = F.mse_loss\n",
    "\n",
    "loss_fn(linear_reg(X, weights, bias), y), mse_loss(linear_reg(X, weights, bias), y)     # same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using nn.Module & nn.Parameter\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearReg()\n",
      "--------------------------------------------------\n",
      "[Parameter containing:\n",
      "tensor([0.0955, 0.0385, 0.0089], requires_grad=True), Parameter containing:\n",
      "tensor([0.6213], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "class LinearReg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # nn.Parameter has requires_grad=True as default\n",
    "        self.weights = nn.Parameter(torch.randn(3) / math.sqrt(len(X)))     # xavier initialization\n",
    "        self.bias = nn.Parameter(torch.randn(1))\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X.matmul(self.weights) + self.bias\n",
    "\n",
    "\n",
    "model = LinearReg()\n",
    "\n",
    "print(model)\n",
    "print('-' * 50)\n",
    "print(list(model.parameters()))     # added to parameters() bc. nn.Parameter is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "epoch 1 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 18928.08984375\n",
      "batch [4 / 6] loss: 39545.0234375\n",
      "batch [6 / 6] loss: 17308.240234375\n",
      "--------------------------------------------------\n",
      "epoch 2 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 953.0055541992188\n",
      "batch [4 / 6] loss: 1121.216064453125\n",
      "batch [6 / 6] loss: 300.2220458984375\n",
      "--------------------------------------------------\n",
      "epoch 3 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 171.156494140625\n",
      "batch [4 / 6] loss: 35.11738586425781\n",
      "batch [6 / 6] loss: 4.091669082641602\n",
      "--------------------------------------------------\n",
      "epoch 4 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 102.42078399658203\n",
      "batch [4 / 6] loss: 2.9902000427246094\n",
      "batch [6 / 6] loss: 25.7353572845459\n",
      "--------------------------------------------------\n",
      "epoch 5 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 92.339111328125\n",
      "batch [4 / 6] loss: 1.7991183996200562\n",
      "batch [6 / 6] loss: 31.214757919311523\n",
      "--------------------------------------------------\n",
      "epoch 6 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 90.35986328125\n",
      "batch [4 / 6] loss: 1.7117235660552979\n",
      "batch [6 / 6] loss: 32.05941390991211\n",
      "--------------------------------------------------\n",
      "epoch 7 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 89.68729400634766\n",
      "batch [4 / 6] loss: 1.6947760581970215\n",
      "batch [6 / 6] loss: 32.07575607299805\n",
      "--------------------------------------------------\n",
      "epoch 8 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 89.2336196899414\n",
      "batch [4 / 6] loss: 1.6863932609558105\n",
      "batch [6 / 6] loss: 31.953433990478516\n",
      "--------------------------------------------------\n",
      "epoch 9 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 88.8177261352539\n",
      "batch [4 / 6] loss: 1.6793941259384155\n",
      "batch [6 / 6] loss: 31.808271408081055\n",
      "--------------------------------------------------\n",
      "epoch 10 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 88.40984344482422\n",
      "batch [4 / 6] loss: 1.6726408004760742\n",
      "batch [6 / 6] loss: 31.65999984741211\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "\n",
    "def train(X_batches, y_batches, model, loss_fn, lr = 0.001, lr_decay = 0.99):\n",
    "    for batch_num, (X, y) in enumerate(zip(X_batches, y_batches)):\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "        with torch.no_grad():   # we don't want these actions to be recorded for next calculation of the gradient\n",
    "            # changed part\n",
    "            for p in model.parameters():\n",
    "                p -= lr * p.grad\n",
    "            model.zero_grad()\n",
    "\n",
    "        lr *= lr_decay\n",
    "\n",
    "        if (batch_num + 1) % 2 == 0:\n",
    "                print(f'batch [{batch_num + 1} / {len(X_batches)}] loss: {loss.item()}')\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('-' * 50)\n",
    "    print(f'epoch {epoch + 1} / {epochs}')\n",
    "    print('-' * 50)\n",
    "\n",
    "    train(X_batches, y_batches, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using nn.Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearReg(\n",
      "  (linear): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n",
      "--------------------------------------------------\n",
      "[Parameter containing:\n",
      "tensor([[ 0.2975, -0.2548, -0.1119]], requires_grad=True), Parameter containing:\n",
      "tensor([0.2710], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "class LinearReg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3,1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(X).squeeze()\n",
    "\n",
    "\n",
    "model = LinearReg()\n",
    "\n",
    "print(model)\n",
    "print('-' * 50)\n",
    "print(list(model.parameters()))     # added to parameters bc. nn.Parameter is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "epoch 1 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 19212.021484375\n",
      "batch [4 / 6] loss: 39723.33203125\n",
      "batch [6 / 6] loss: 16121.7109375\n",
      "--------------------------------------------------\n",
      "epoch 2 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 850.8599853515625\n",
      "batch [4 / 6] loss: 924.115478515625\n",
      "batch [6 / 6] loss: 207.44674682617188\n",
      "--------------------------------------------------\n",
      "epoch 3 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 161.54225158691406\n",
      "batch [4 / 6] loss: 25.274185180664062\n",
      "batch [6 / 6] loss: 7.740100860595703\n",
      "--------------------------------------------------\n",
      "epoch 4 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 104.6955337524414\n",
      "batch [4 / 6] loss: 2.715043783187866\n",
      "batch [6 / 6] loss: 27.675636291503906\n",
      "--------------------------------------------------\n",
      "epoch 5 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 96.83209228515625\n",
      "batch [4 / 6] loss: 1.885658860206604\n",
      "batch [6 / 6] loss: 31.692121505737305\n",
      "--------------------------------------------------\n",
      "epoch 6 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 95.295166015625\n",
      "batch [4 / 6] loss: 1.8131483793258667\n",
      "batch [6 / 6] loss: 32.1927375793457\n",
      "--------------------------------------------------\n",
      "epoch 7 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 94.68914794921875\n",
      "batch [4 / 6] loss: 1.79722261428833\n",
      "batch [6 / 6] loss: 32.13875198364258\n",
      "--------------------------------------------------\n",
      "epoch 8 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 94.22472381591797\n",
      "batch [4 / 6] loss: 1.7885136604309082\n",
      "batch [6 / 6] loss: 32.00051498413086\n",
      "--------------------------------------------------\n",
      "epoch 9 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 93.7833251953125\n",
      "batch [4 / 6] loss: 1.7809014320373535\n",
      "batch [6 / 6] loss: 31.85107421875\n",
      "--------------------------------------------------\n",
      "epoch 10 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 93.3470687866211\n",
      "batch [4 / 6] loss: 1.7734776735305786\n",
      "batch [6 / 6] loss: 31.700014114379883\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "\n",
    "def train(X_batches, y_batches, model, loss_fn, lr = 0.001, lr_decay = 0.99):   # same train function as before\n",
    "    for batch_num, (X, y) in enumerate(zip(X_batches, y_batches)):\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            for p in model.parameters():\n",
    "                p -= lr * p.grad\n",
    "            model.zero_grad()\n",
    "\n",
    "        lr *= lr_decay\n",
    "\n",
    "        if (batch_num + 1) % 2 == 0:\n",
    "                print(f'batch [{batch_num + 1} / {len(X_batches)}] loss: {loss.item()}')\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('-' * 50)\n",
    "    print(f'epoch {epoch + 1} / {epochs}')\n",
    "    print('-' * 50)\n",
    "\n",
    "    train(X_batches, y_batches, model, loss_fn, lr_decay=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using torch.optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "epoch 1 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 19212.021484375\n",
      "batch [4 / 6] loss: 39723.33203125\n",
      "batch [6 / 6] loss: 16121.7109375\n",
      "--------------------------------------------------\n",
      "epoch 2 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 850.8602294921875\n",
      "batch [4 / 6] loss: 924.115478515625\n",
      "batch [6 / 6] loss: 207.44674682617188\n",
      "--------------------------------------------------\n",
      "epoch 3 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 161.54229736328125\n",
      "batch [4 / 6] loss: 25.274185180664062\n",
      "batch [6 / 6] loss: 7.740100860595703\n",
      "--------------------------------------------------\n",
      "epoch 4 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 104.6955337524414\n",
      "batch [4 / 6] loss: 2.715043783187866\n",
      "batch [6 / 6] loss: 27.675636291503906\n",
      "--------------------------------------------------\n",
      "epoch 5 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 96.83209228515625\n",
      "batch [4 / 6] loss: 1.885658860206604\n",
      "batch [6 / 6] loss: 31.692121505737305\n",
      "--------------------------------------------------\n",
      "epoch 6 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 95.29519653320312\n",
      "batch [4 / 6] loss: 1.8131604194641113\n",
      "batch [6 / 6] loss: 32.1927375793457\n",
      "--------------------------------------------------\n",
      "epoch 7 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 94.68914794921875\n",
      "batch [4 / 6] loss: 1.79722261428833\n",
      "batch [6 / 6] loss: 32.13875198364258\n",
      "--------------------------------------------------\n",
      "epoch 8 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 94.22472381591797\n",
      "batch [4 / 6] loss: 1.7885146141052246\n",
      "batch [6 / 6] loss: 32.00051498413086\n",
      "--------------------------------------------------\n",
      "epoch 9 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 93.7833480834961\n",
      "batch [4 / 6] loss: 1.7809125185012817\n",
      "batch [6 / 6] loss: 31.85107421875\n",
      "--------------------------------------------------\n",
      "epoch 10 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 93.3470687866211\n",
      "batch [4 / 6] loss: 1.7734776735305786\n",
      "batch [6 / 6] loss: 31.699960708618164\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "model = LinearReg()     # same model as before\n",
    "optimizer = SGD(model.parameters(), lr=0.001)\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "def train(X_batches, y_batches, model, loss_fn, optimizer):\n",
    "    for batch_num, (X, y) in enumerate(zip(X_batches, y_batches)):\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "        # changed part\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if (batch_num + 1) % 2 == 0:\n",
    "                print(f'batch [{batch_num + 1} / {len(X_batches)}] loss: {loss.item()}')\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('-' * 50)\n",
    "    print(f'epoch {epoch + 1} / {epochs}')\n",
    "    print('-' * 50)\n",
    "\n",
    "    train(X_batches, y_batches, model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Datasets & Dataset\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "\n",
    "https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([1., 1., 1.]), tensor(55.2372)),\n",
       " (tensor([[10.4000, 10.4000, 10.4000]]), tensor([401.6051])))"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, TensorDataset\n",
    "\n",
    "# choice 1\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return X[index, :], y[index]\n",
    "\n",
    "\n",
    "# dataset = CustomDataset(X, y)\n",
    "\n",
    "# choice 2\n",
    "dataset = TensorDataset(X,y)\n",
    "\n",
    "dataset[0], dataset[-1:]    # indexing & slicing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 48)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size, shuffle=True)  # don't need to shuffle for validation data\n",
    "\n",
    "len(dataloader), len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "epoch 1 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 43860.66015625\n",
      "batch [4 / 6] loss: 6618.982421875\n",
      "batch [6 / 6] loss: 1878.8720703125\n",
      "--------------------------------------------------\n",
      "epoch 2 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 701.1918334960938\n",
      "batch [4 / 6] loss: 560.6073608398438\n",
      "batch [6 / 6] loss: 179.83984375\n",
      "--------------------------------------------------\n",
      "epoch 3 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 68.46882629394531\n",
      "batch [4 / 6] loss: 42.12972640991211\n",
      "batch [6 / 6] loss: 62.00067138671875\n",
      "--------------------------------------------------\n",
      "epoch 4 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 69.39556121826172\n",
      "batch [4 / 6] loss: 26.955913543701172\n",
      "batch [6 / 6] loss: 51.885169982910156\n",
      "--------------------------------------------------\n",
      "epoch 5 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 62.75263977050781\n",
      "batch [4 / 6] loss: 51.94991683959961\n",
      "batch [6 / 6] loss: 66.76713562011719\n",
      "--------------------------------------------------\n",
      "epoch 6 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 36.80405807495117\n",
      "batch [4 / 6] loss: 37.67058563232422\n",
      "batch [6 / 6] loss: 41.95133972167969\n",
      "--------------------------------------------------\n",
      "epoch 7 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 36.13618469238281\n",
      "batch [4 / 6] loss: 67.21483612060547\n",
      "batch [6 / 6] loss: 51.062042236328125\n",
      "--------------------------------------------------\n",
      "epoch 8 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 62.1530647277832\n",
      "batch [4 / 6] loss: 27.131025314331055\n",
      "batch [6 / 6] loss: 89.56642150878906\n",
      "--------------------------------------------------\n",
      "epoch 9 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 50.74446487426758\n",
      "batch [4 / 6] loss: 34.15101623535156\n",
      "batch [6 / 6] loss: 39.06275177001953\n",
      "--------------------------------------------------\n",
      "epoch 10 / 10\n",
      "--------------------------------------------------\n",
      "batch [2 / 6] loss: 63.56093215942383\n",
      "batch [4 / 6] loss: 67.2801513671875\n",
      "batch [6 / 6] loss: 55.452178955078125\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "model = LinearReg()     # same model as before\n",
    "optimizer = SGD(model.parameters(), lr=0.001)\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    for batch_num, (X, y) in enumerate(dataloader):     # changed part\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if (batch_num + 1) % 2 == 0:\n",
    "                print(f'batch [{batch_num + 1} / {len(X_batches)}] loss: {loss.item()}')\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('-' * 50)\n",
    "    print(f'epoch {epoch + 1} / {epochs}')\n",
    "    print('-' * 50)\n",
    "\n",
    "    train(dataloader, model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using nn.Sequential\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html\n",
    "\n",
    "nn.Sequential only takes instances of nn.Module as parameters -> we need to be able to make custom layers to take advantage of nn.Sequential\n",
    "\n",
    "nn.Sequential treats the whole container as a single module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Lambda()\n",
       "  (1): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (2): Lambda()\n",
       ")"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create custom layer\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func) -> None:\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "\n",
    "\n",
    "model = nn.Sequential(Lambda(lambda x: x.view(-1, 1, 28, 28)),  # a custom view layer can be added to nn.Sequential\n",
    "                      nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "                      Lambda(lambda x: x.view(x.size(0), -1)))\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 3, 3])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())[0].shape   # kernels of CNN layer (out channels, in channel, kernel size, kernel size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn.ModuleList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net() []\n",
      "Error: model.parameters() is empty, training is not possible\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "layer_list = [nn.Linear(3,3), nn.Linear(3,1)]\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, layer_list) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layer_list    # parameters of Modules in layer_list are not registered to model.parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x.squeeze()\n",
    "\n",
    "\n",
    "model = Net(layer_list)\n",
    "print(model, list(model.parameters()))\n",
    "\n",
    "try:\n",
    "    optimizer = SGD(model.parameters(), lr=0.001)   # training won't work bc. model.parameters() is empty\n",
    "except:\n",
    "    print('Error: model.parameters() is empty, training is not possible')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=3, bias=True)\n",
      "    (1): Linear(in_features=3, out_features=1, bias=True)\n",
      "  )\n",
      ") [Parameter containing:\n",
      "tensor([[ 0.2975, -0.2548, -0.1119],\n",
      "        [ 0.2710, -0.5435,  0.3462],\n",
      "        [-0.1188,  0.2937,  0.0803]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.0707,  0.1601,  0.0285], requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.2109, -0.2250, -0.0421]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.0520], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "layer_list = [nn.Linear(3,3), nn.Linear(3,1)]\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, layer_list) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(layer_list)    # parameters are registered to model.parameters() by using nn.ModuleList\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x.squeeze()\n",
    "\n",
    "\n",
    "model = Net(layer_list)\n",
    "print(model, list(model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Printing Information about Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('layers.0.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.2975, -0.2548, -0.1119],\n",
       "          [ 0.2710, -0.5435,  0.3462],\n",
       "          [-0.1188,  0.2937,  0.0803]], requires_grad=True)),\n",
       " ('layers.0.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0707,  0.1601,  0.0285], requires_grad=True)),\n",
       " ('layers.1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.2109, -0.2250, -0.0421]], requires_grad=True)),\n",
       " ('layers.1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0520], requires_grad=True))]"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('',\n",
       "  Net(\n",
       "    (layers): ModuleList(\n",
       "      (0): Linear(in_features=3, out_features=3, bias=True)\n",
       "      (1): Linear(in_features=3, out_features=1, bias=True)\n",
       "    )\n",
       "  )),\n",
       " ('layers',\n",
       "  ModuleList(\n",
       "    (0): Linear(in_features=3, out_features=3, bias=True)\n",
       "    (1): Linear(in_features=3, out_features=1, bias=True)\n",
       "  )),\n",
       " ('layers.0', Linear(in_features=3, out_features=3, bias=True)),\n",
       " ('layers.1', Linear(in_features=3, out_features=1, bias=True))]"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.named_modules())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('layers',\n",
       "  ModuleList(\n",
       "    (0): Linear(in_features=3, out_features=3, bias=True)\n",
       "    (1): Linear(in_features=3, out_features=1, bias=True)\n",
       "  ))]"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.named_children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.named_buffers())     # return buffer tensors such as running mean average of a Batch Norm layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Learning Rates for Different layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (linear1): Linear(in_features=1, out_features=3, bias=True)\n",
       "  (linear2): Linear(in_features=3, out_features=2, bias=True)\n",
       "  (layer): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=3, bias=True)\n",
       "    (1): Linear(in_features=3, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(1,3)\n",
    "        self.linear2 = nn.Linear(3,2)\n",
    "        self.layer = nn.Sequential(nn.Linear(2,3),\n",
    "                                   nn.Linear(3,1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "    \n",
    "model = Net()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.1983],\n",
       "         [-0.3121],\n",
       "         [ 0.1697]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.7738,  0.1432, -0.4728], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[0.5735, 0.0123, 0.4564],\n",
       "         [0.1407, 0.5175, 0.3331]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.2943,  0.4056], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0045, -0.5676],\n",
       "         [-0.1482, -0.6382],\n",
       "         [-0.3664, -0.5009]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.4836, -0.1486, -0.1222], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.2767,  0.2350, -0.3810]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.2128], requires_grad=True)]"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('linear1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.1983],\n",
       "          [-0.3121],\n",
       "          [ 0.1697]], requires_grad=True)),\n",
       " ('linear1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.7738,  0.1432, -0.4728], requires_grad=True)),\n",
       " ('linear2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[0.5735, 0.0123, 0.4564],\n",
       "          [0.1407, 0.5175, 0.3331]], requires_grad=True)),\n",
       " ('linear2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.2943,  0.4056], requires_grad=True)),\n",
       " ('layer.0.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0045, -0.5676],\n",
       "          [-0.1482, -0.6382],\n",
       "          [-0.3664, -0.5009]], requires_grad=True)),\n",
       " ('layer.0.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.4836, -0.1486, -0.1222], requires_grad=True)),\n",
       " ('layer.1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.2767,  0.2350, -0.3810]], requires_grad=True)),\n",
       " ('layer.1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([0.2128], requires_grad=True))]"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear1.weight',\n",
       "              tensor([[-0.1983],\n",
       "                      [-0.3121],\n",
       "                      [ 0.1697]])),\n",
       "             ('linear1.bias', tensor([ 0.7738,  0.1432, -0.4728])),\n",
       "             ('linear2.weight',\n",
       "              tensor([[0.5735, 0.0123, 0.4564],\n",
       "                      [0.1407, 0.5175, 0.3331]])),\n",
       "             ('linear2.bias', tensor([-0.2943,  0.4056])),\n",
       "             ('layer.0.weight',\n",
       "              tensor([[ 0.0045, -0.5676],\n",
       "                      [-0.1482, -0.6382],\n",
       "                      [-0.3664, -0.5009]])),\n",
       "             ('layer.0.bias', tensor([ 0.4836, -0.1486, -0.1222])),\n",
       "             ('layer.1.weight', tensor([[ 0.2767,  0.2350, -0.3810]])),\n",
       "             ('layer.1.bias', tensor([0.2128]))])"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    differentiable: False\n",
       "    foreach: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 1\n",
       "    dampening: 0\n",
       "    differentiable: False\n",
       "    foreach: None\n",
       "    lr: 0.0001\n",
       "    maximize: False\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# different learning rate for different layers\n",
    "\n",
    "optim1 = SGD([{'params':model.linear1.parameters(), 'lr':0.001},\n",
    "              {'params':model.layer.parameters()}],   # since lr is not specified for this group of parameters, use the lr given as input to optimizer\n",
    "              lr=0.0001)\n",
    "\n",
    "optim1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    differentiable: False\n",
       "    foreach: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 1\n",
       "    dampening: 0\n",
       "    differentiable: False\n",
       "    foreach: None\n",
       "    lr: 0.0001\n",
       "    maximize: False\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# different learning rate for weights and bias\n",
    "\n",
    "optim2 = SGD([{'params':(param for name, param in model.named_parameters() if 'weight' in name), 'lr':0.001},\n",
    "              {'params':(param for name, param in model.named_parameters() if 'bias' in name)}],\n",
    "              lr=0.0001)\n",
    "\n",
    "optim2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': {},\n",
       " 'param_groups': [{'lr': 0.001,\n",
       "   'momentum': 0,\n",
       "   'dampening': 0,\n",
       "   'weight_decay': 0,\n",
       "   'nesterov': False,\n",
       "   'maximize': False,\n",
       "   'foreach': None,\n",
       "   'differentiable': False,\n",
       "   'params': [0, 1, 2, 3]},\n",
       "  {'lr': 0.0001,\n",
       "   'momentum': 0,\n",
       "   'dampening': 0,\n",
       "   'weight_decay': 0,\n",
       "   'nesterov': False,\n",
       "   'maximize': False,\n",
       "   'foreach': None,\n",
       "   'differentiable': False,\n",
       "   'params': [4, 5, 6, 7]}]}"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optim2.state_dict()     # hyperparameters of optimizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
