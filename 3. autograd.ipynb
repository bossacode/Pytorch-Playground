{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=MswxJw-8PvE\n",
    "\n",
    "https://pytorch.org/docs/stable/notes/extending.html#extending-autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# autograd\n",
    "\n",
    "https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/\n",
    "\n",
    "https://pytorch.org/docs/stable/autograd.html\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html\n",
    "\n",
    "https://pytorch.org/docs/stable/notes/autograd.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.autograd.Function\n",
    "\n",
    "https://pytorch.org/docs/stable/autograd.html#function\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/examples_autograd/polynomial_custom_function.html#sphx-glr-beginner-examples-autograd-polynomial-custom-function-py\n",
    "\n",
    "https://hongl.tistory.com/206"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All functions(mathematical operations) applied to tensors to construct computational graph are an object of torch.autograd.Function class.\n",
    "\n",
    "Any subclasses of this class  forward() function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double Backward\n",
    "https://pytorch.org/tutorials/intermediate/custom_function_double_backward_tutorial.html#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x7f71138e54f0>\n",
      "((<MmBackward0 object at 0x7f71135145e0>, 0), (<MmBackward0 object at 0x7f7113935af0>, 0))\n",
      "((None, 0), (<AccumulateGrad object at 0x7f71138e54f0>, 0))\n",
      "((None, 0), (<AccumulateGrad object at 0x7f711381aa00>, 0))\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,4); w1 = torch.randn(4,2, requires_grad=True)\n",
    "y = torch.randn(3,4); w2 = torch.randn(4,2, requires_grad=True)\n",
    "\n",
    "out = x.matmul(w1) + y.matmul(w2)\n",
    "l = out.sum()\n",
    "\n",
    "print(out.grad_fn)\n",
    "print(out.grad_fn.next_functions)\n",
    "print(out.grad_fn.next_functions[0][0].next_functions)\n",
    "print(out.grad_fn.next_functions[1][0].next_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SumBackward0 object at 0x7f71134f3580>\n",
      "((<AddBackward0 object at 0x7f7113608310>, 0),)\n",
      "((<MmBackward0 object at 0x7f71137735b0>, 0), (<MmBackward0 object at 0x7f7113773eb0>, 0))\n"
     ]
    }
   ],
   "source": [
    "print(l.grad_fn)\n",
    "print(l.grad_fn.next_functions)\n",
    "print(l.grad_fn.next_functions[0][0].next_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x7f7156f5deb0>\n",
      "((<AddBackward0 object at 0x7f7113865ee0>, 0), (<MmBackward0 object at 0x7f71135edeb0>, 0))\n",
      "((<MmBackward0 object at 0x7f7156f5deb0>, 0), (<MmBackward0 object at 0x7f71135edeb0>, 0))\n",
      "((None, 0), (<AccumulateGrad object at 0x7f71134f7c70>, 0))\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,4); w1 = torch.randn(4,2, requires_grad=True)\n",
    "y = torch.randn(3,4); w2 = torch.randn(4,2, requires_grad=True)\n",
    "z = torch.randn(3,4); w3 = torch.randn(4,2, requires_grad=True)\n",
    "\n",
    "out = x.matmul(w1) + y.matmul(w2) + z.matmul(w3)\n",
    "l = out.sum()\n",
    "\n",
    "print(out.grad_fn)\n",
    "print(out.grad_fn.next_functions)\n",
    "print(out.grad_fn.next_functions[0][0].next_functions)\n",
    "print(out.grad_fn.next_functions[1][0].next_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SumBackward0 object at 0x7f711573b6d0>\n",
      "((<AddBackward0 object at 0x7f711573b700>, 0),)\n",
      "((<AddBackward0 object at 0x7f711573b6d0>, 0), (<MmBackward0 object at 0x7f711637f310>, 0))\n",
      "((<MmBackward0 object at 0x7f711573b820>, 0), (<MmBackward0 object at 0x7f711573b700>, 0))\n"
     ]
    }
   ],
   "source": [
    "print(l.grad_fn)\n",
    "print(l.grad_fn.next_functions)\n",
    "print(l.grad_fn.next_functions[0][0].next_functions)\n",
    "print(l.grad_fn.next_functions[0][0].next_functions[0][0].next_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# grad_fn\n",
    "\n",
    "https://amsword.medium.com/understanding-pytorchs-autograd-with-grad-fn-and-next-functions-b2c4836daa00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "tensor([0.0000, 0.6981, 1.3963, 2.0944, 2.7925, 3.4907, 4.1888, 4.8869, 5.5851,\n",
      "        6.2832], requires_grad=True) \n",
      "\n",
      "w = sin(x):\n",
      "tensor([ 0.0000e+00,  6.4279e-01,  9.8481e-01,  8.6603e-01,  3.4202e-01,\n",
      "        -3.4202e-01, -8.6603e-01, -9.8481e-01, -6.4279e-01,  1.7485e-07],\n",
      "       grad_fn=<SinBackward0>) \n",
      "\n",
      "y = 2 * w:\n",
      "tensor([ 0.0000e+00,  1.2856e+00,  1.9696e+00,  1.7321e+00,  6.8404e-01,\n",
      "        -6.8404e-01, -1.7321e+00, -1.9696e+00, -1.2856e+00,  3.4969e-07],\n",
      "       grad_fn=<MulBackward0>) \n",
      "\n",
      "z = y + 1:\n",
      "tensor([ 1.0000,  2.2856,  2.9696,  2.7321,  1.6840,  0.3160, -0.7321, -0.9696,\n",
      "        -0.2856,  1.0000], grad_fn=<AddBackward0>) \n",
      "\n",
      "l = sum(z):\n",
      "tensor(10.0000, grad_fn=<SumBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# make data\n",
    "\n",
    "x = torch.linspace(0., 2 * math.pi, steps=10, requires_grad=True)\n",
    "print('x:')\n",
    "print(x, '\\n')\n",
    "\n",
    "# 1\n",
    "print('w = sin(x):')\n",
    "w = torch.sin(x)\n",
    "print(w, '\\n')    # has grad_fn\n",
    "\n",
    "# 2\n",
    "print('y = 2 * w:')\n",
    "y = 2 * w\n",
    "print(y, '\\n')\n",
    "\n",
    "# 3\n",
    "print('z = y + 1:')\n",
    "z = y + 1\n",
    "print(z, '\\n')\n",
    "\n",
    "# 4\n",
    "print('l = sum(z):')\n",
    "l = z.sum()\n",
    "print(l, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dl/dl: tensor(1.)\n",
      "--------------------------------------------------\n",
      "l.grad_fn: <SumBackward0 object at 0x7fd2fae52670>\n",
      "dl/dz: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "--------------------------------------------------\n",
      "z.grad_fn: ((<AddBackward0 object at 0x7fd2fadf53d0>, 0),)\n",
      "dl/dy: (tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), None)\n",
      "--------------------------------------------------\n",
      "y.grad_fn: ((<MulBackward0 object at 0x7fd2fadf53d0>, 0), (None, 0))\n",
      "dl/dw: (tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]), None)\n",
      "--------------------------------------------------\n",
      "w.grad_fn: ((<SinBackward0 object at 0x7fd2ff0b2fd0>, 0), (None, 0))\n",
      "dl/dx: tensor([ 2.0000,  1.5321,  0.3473, -1.0000, -1.8794, -1.8794, -1.0000,  0.3473,\n",
      "         1.5321,  2.0000], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# gradient history of l\n",
    "\n",
    "dl = torch.tensor(1.)   # dl/dl\n",
    "print('dl/dl:', dl)\n",
    "\n",
    "print('-' * 50)\n",
    "\n",
    "print('l.grad_fn:', l.grad_fn)\n",
    "dz = l.grad_fn(dl)      # dl/dz = dl/dz * dl/dl\n",
    "print('dl/dz:', dz)\n",
    "\n",
    "print('-' * 50)\n",
    "\n",
    "print('z.grad_fn:', l.grad_fn.next_functions)\n",
    "dy = z.grad_fn(dz)      # dl/dy = dz/dy * dl/dz * dl/dl\n",
    "print('dl/dy:', dy)\n",
    "\n",
    "print('-' * 50)\n",
    "\n",
    "print('y.grad_fn:', l.grad_fn.next_functions[0][0].next_functions)\n",
    "dw = y.grad_fn(dy[0])   # dl/dw = dy/dw * dz/dy * dl/dz * dl/dl\n",
    "print('dl/dw:', dw)\n",
    "\n",
    "print('-' * 50)\n",
    "\n",
    "print('w.grad_fn:', l.grad_fn.next_functions[0][0].next_functions[0][0].next_functions)\n",
    "dx = w.grad_fn(dw[0])   # dl/dx = dw/dx * dy/dw * dz/dy * dl/dz * dl/dl\n",
    "print('dl/dx:', dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((<AccumulateGrad at 0x7fd2faf69ee0>, 0),)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dl/dx: tensor([ 2.0000,  1.5321,  0.3473, -1.0000, -1.8794, -1.8794, -1.0000,  0.3473,\n",
      "         1.5321,  2.0000])\n"
     ]
    }
   ],
   "source": [
    "l.backward()\n",
    "print('dl/dx:', x.grad)     # same gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.is_leaf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
